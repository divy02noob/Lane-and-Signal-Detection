{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":828,"status":"ok","timestamp":1742535248339,"user":{"displayName":"Gray Fullbuster","userId":"03818290908344673962"},"user_tz":-330},"id":"elzDE-Ov3ThI","outputId":"f6afe517-e5c0-4a21-ae9e-c0fa6316ba7c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Video processing complete. Output saved as 'output_traffic_light_detection.mp4'\n"]}],"source":["import cv2\n","import numpy as np\n","\n","# Define HSV color range for each light\n","def define_color_ranges():\n","    lower_red1 = np.array([0, 100, 100])\n","    upper_red1 = np.array([10, 255, 255])\n","    lower_red2 = np.array([160, 100, 100])\n","    upper_red2 = np.array([180, 255, 255])\n","\n","    lower_yellow = np.array([20, 150, 150])  # Adjusted yellow lower bounds\n","    upper_yellow = np.array([40, 255, 255])\n","\n","    lower_green = np.array([40, 50, 50])\n","    upper_green = np.array([90, 255, 255])\n","\n","    return (lower_red1, upper_red1), (lower_red2, upper_red2), (lower_yellow, upper_yellow), (lower_green, upper_green)\n","\n","# Function to detect the color of the traffic light and return contours\n","def detect_traffic_light(frame):\n","    hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n","\n","    (lower_red1, upper_red1), (lower_red2, upper_red2), (lower_yellow, upper_yellow), (lower_green, upper_green) = define_color_ranges()\n","\n","    red_mask1 = cv2.inRange(hsv_frame, lower_red1, upper_red1)\n","    red_mask2 = cv2.inRange(hsv_frame, lower_red2, upper_red2)\n","    red_mask = red_mask1 | red_mask2\n","    yellow_mask = cv2.inRange(hsv_frame, lower_yellow, upper_yellow)\n","    green_mask = cv2.inRange(hsv_frame, lower_green, upper_green)\n","\n","    kernel = np.ones((5, 5), np.uint8)\n","    red_mask = cv2.erode(red_mask, kernel, iterations=1)\n","    red_mask = cv2.dilate(red_mask, kernel, iterations=1)\n","\n","    green_mask = cv2.erode(green_mask, kernel, iterations=1)\n","    green_mask = cv2.dilate(green_mask, kernel, iterations=1)\n","\n","    yellow_mask = cv2.erode(yellow_mask, kernel, iterations=1)\n","    yellow_mask = cv2.dilate(yellow_mask, kernel, iterations=1)\n","\n","    contours_red, _ = cv2.findContours(red_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n","    contours_yellow, _ = cv2.findContours(yellow_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n","    contours_green, _ = cv2.findContours(green_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n","\n","    color = \"None\"\n","    contours = None\n","\n","    if len(contours_red) > 0 and max([cv2.contourArea(c) for c in contours_red]) > 500: #reduced area for better detection\n","        color = \"Red, Stop\"\n","        contours = contours_red\n","    elif len(contours_yellow) > 0 and max([cv2.contourArea(c) for c in contours_yellow]) > 500: #reduced area for better detection\n","        color = \"Yellow, Slow Down\"\n","        contours = contours_yellow\n","    elif len(contours_green) > 0 and max([cv2.contourArea(c) for c in contours_green]) > 500: #reduced area for better detection\n","        color = \"Green, Go\"\n","        contours = contours_green\n","\n","    return color, contours\n","\n","# Function to draw a square around the detected traffic light\n","def draw_square(frame, color, contours):\n","    if contours:\n","        largest_contour = max(contours, key=cv2.contourArea)\n","        x, y, w, h = cv2.boundingRect(largest_contour)\n","\n","        if color == \"Red, Stop\":\n","            box_color = (0, 0, 255)\n","        elif color == \"Yellow, Slow Down\":\n","            box_color = (0, 255, 255)\n","        elif color == \"Green, Go\":\n","            box_color = (0, 255, 0)\n","        else:\n","            box_color = (255, 255, 255)\n","\n","        cv2.rectangle(frame, (x, y), (x+w, y+h), box_color, 5)\n","\n","    return frame\n","\n","# Main function to read from video file and detect traffic light\n","def main(video_path):\n","    cap = cv2.VideoCapture(video_path)\n","\n","    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","    fps = int(cap.get(cv2.CAP_PROP_FPS))\n","\n","    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","    out = cv2.VideoWriter('output_traffic_light_detection.mp4', fourcc, fps, (width, height))\n","\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        traffic_light_color, contours = detect_traffic_light(frame)\n","\n","        frame = draw_square(frame, traffic_light_color, contours)\n","\n","        cv2.putText(frame, f'Traffic Light: {traffic_light_color}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n","\n","        out.write(frame)\n","\n","    cap.release()\n","    out.release()\n","    print(\"Video processing complete. Output saved as 'output_traffic_light_detection.mp4'\")\n","\n","if __name__ == \"__main__\":\n","    video_path = '/content/21165-315405392_tiny.mp4'  # Replace with your video file path\n","    main(video_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZUOMN0ON1WNW"},"outputs":[],"source":["import cv2\n","import numpy as np\n","from google.colab.patches import cv2_imshow # Import cv2_imshow\n","\n","\n","def detect_lanes(frame):\n","    # Convert the frame to grayscale\n","    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","\n","    # Apply Gaussian blur to reduce noise\n","    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n","\n","    # Detect edges using Canny edge detector\n","    edges = cv2.Canny(blurred, 50, 150)\n","\n","    # Define a region of interest (ROI)\n","    height, width = edges.shape\n","    roi_vertices = [\n","        (0, height),\n","        (width // 2, height // 2),  # Adjust this point for your ROI\n","        (width, height),\n","    ]\n","    mask = np.zeros_like(edges)\n","    cv2.fillPoly(mask, [np.array(roi_vertices, np.int32)], 255)\n","    masked_edges = cv2.bitwise_and(edges, mask)\n","\n","    # Apply Hough transform to detect lines\n","    lines = cv2.HoughLinesP(\n","        masked_edges,\n","        rho=2,\n","        theta=np.pi / 180,\n","        threshold=100,  # Adjust this threshold\n","        minLineLength=40,  # Adjust this value\n","        maxLineGap=20,  # Adjust this value\n","    )\n","\n","    # Draw the detected lines on the original frame\n","    line_image = np.zeros_like(frame)\n","    if lines is not None:\n","        for line in lines:\n","            x1, y1, x2, y2 = line[0]\n","            cv2.line(line_image, (x1, y1), (x2, y2), (0, 0, 255), 5)\n","\n","    # Combine the line image with the original frame\n","    result = cv2.addWeighted(frame, 0.8, line_image, 1, 0)\n","\n","    return result\n","\n","# Load the video\n","video_path = \"/content/80395-572395743_tiny.mp4\"  # Replace with your video path\n","cap = cv2.VideoCapture(video_path)\n","\n","while cap.isOpened():\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","\n","    # Detect lanes in the frame\n","    processed_frame = detect_lanes(frame)\n","\n","    # Display the processed frame using cv2_imshow\n","    cv2_imshow(processed_frame) # Use cv2_imshow instead of cv2.imshow\n","\n","\n","cap.release()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":255,"status":"ok","timestamp":1747118585572,"user":{"displayName":"Darsh Verma","userId":"04546128917656418162"},"user_tz":-330},"id":"H8q8KF8oodxu","outputId":"039bda8c-fa20-4dfb-ff96-ea7e7cdd0040"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing complete. Output saved as 'output_combined_detection.mp4'\n"]}],"source":["import cv2\n","import numpy as np\n","\n","# Define HSV color range for traffic lights\n","def define_color_ranges():\n","    lower_red1 = np.array([0, 100, 100])\n","    upper_red1 = np.array([10, 255, 255])\n","    lower_red2 = np.array([160, 100, 100])\n","    upper_red2 = np.array([180, 255, 255])\n","\n","    lower_yellow = np.array([20, 150, 150])\n","    upper_yellow = np.array([40, 255, 255])\n","\n","    lower_green = np.array([40, 50, 50])\n","    upper_green = np.array([90, 255, 255])\n","\n","    return (lower_red1, upper_red1), (lower_red2, upper_red2), (lower_yellow, upper_yellow), (lower_green, upper_green)\n","\n","# Detect traffic lights\n","def detect_traffic_light(frame):\n","    hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n","    (lr1, ur1), (lr2, ur2), (ly, uy), (lg, ug) = define_color_ranges()\n","\n","    red_mask = cv2.inRange(hsv_frame, lr1, ur1) | cv2.inRange(hsv_frame, lr2, ur2)\n","    yellow_mask = cv2.inRange(hsv_frame, ly, uy)\n","    green_mask = cv2.inRange(hsv_frame, lg, ug)\n","\n","    kernel = np.ones((5, 5), np.uint8)\n","    red_mask = cv2.dilate(cv2.erode(red_mask, kernel, iterations=1), kernel, iterations=1)\n","    yellow_mask = cv2.dilate(cv2.erode(yellow_mask, kernel, iterations=1), kernel, iterations=1)\n","    green_mask = cv2.dilate(cv2.erode(green_mask, kernel, iterations=1), kernel, iterations=1)\n","\n","    contours_red, _ = cv2.findContours(red_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n","    contours_yellow, _ = cv2.findContours(yellow_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n","    contours_green, _ = cv2.findContours(green_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n","\n","    color = \"None\"\n","    contours = None\n","\n","    if contours_red and max([cv2.contourArea(c) for c in contours_red]) > 500:\n","        color, contours = \"Red, Stop\", contours_red\n","    elif contours_yellow and max([cv2.contourArea(c) for c in contours_yellow]) > 500:\n","        color, contours = \"Yellow, Slow Down\", contours_yellow\n","    elif contours_green and max([cv2.contourArea(c) for c in contours_green]) > 500:\n","        color, contours = \"Green, Go\", contours_green\n","\n","    return color, contours\n","\n","# Draw bounding box for traffic light\n","def draw_traffic_light_box(frame, color, contours):\n","    if contours:\n","        largest = max(contours, key=cv2.contourArea)\n","        x, y, w, h = cv2.boundingRect(largest)\n","        colors = {\n","            \"Red, Stop\": (0, 0, 255),\n","            \"Yellow, Slow Down\": (0, 255, 255),\n","            \"Green, Go\": (0, 255, 0),\n","            \"None\": (255, 255, 255)\n","        }\n","        cv2.rectangle(frame, (x, y), (x+w, y+h), colors.get(color, (255, 255, 255)), 5)\n","    return frame\n","\n","# Lane detection\n","def detect_lanes(frame):\n","    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n","    edges = cv2.Canny(blurred, 50, 150)\n","\n","    height, width = edges.shape\n","    roi_vertices = [(0, height), (width // 2, height // 2), (width, height)]\n","    mask = np.zeros_like(edges)\n","    cv2.fillPoly(mask, [np.array(roi_vertices, np.int32)], 255)\n","    masked_edges = cv2.bitwise_and(edges, mask)\n","\n","    lines = cv2.HoughLinesP(masked_edges, 2, np.pi / 180, 100, minLineLength=40, maxLineGap=20)\n","    line_image = np.zeros_like(frame)\n","\n","    if lines is not None:\n","        for line in lines:\n","            x1, y1, x2, y2 = line[0]\n","            cv2.line(line_image, (x1, y1), (x2, y2), (0, 0, 255), 5)\n","\n","    result = cv2.addWeighted(frame, 0.8, line_image, 1, 0)\n","    return result\n","\n","# Main video processing function\n","def main(video_path):\n","    cap = cv2.VideoCapture(video_path)\n","    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","    fps = int(cap.get(cv2.CAP_PROP_FPS))\n","\n","    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","    out = cv2.VideoWriter('output_combined_detection.mp4', fourcc, fps, (width, height))\n","\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        traffic_light_color, contours = detect_traffic_light(frame)\n","        frame = draw_traffic_light_box(frame, traffic_light_color, contours)\n","        frame = detect_lanes(frame)\n","\n","        cv2.putText(frame, f'Traffic Light: {traffic_light_color}', (10, 30),\n","                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n","\n","        out.write(frame)\n","\n","    cap.release()\n","    out.release()\n","    print(\"Processing complete. Output saved as 'output_combined_detection.mp4'\")\n","\n","# Example usage\n","if __name__ == \"__main__\":\n","    video_path = '/content/sample_video.mp4'  # Replace with your video path\n","    main(video_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cl40GsNgaV_u","outputId":"6120486b-f149-4169-a2d2-e8eba0e284cf","executionInfo":{"status":"ok","timestamp":1747131455217,"user_tz":-330,"elapsed":49,"user":{"displayName":"Divyansh Srivastava","userId":"06708854556260485286"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting app.py\n"]}],"source":["%%writefile app.py\n","import streamlit as st\n","import cv2\n","import numpy as np\n","from PIL import Image\n","import tempfile\n","import os\n","\n","# Define HSV color ranges\n","def define_color_ranges():\n","    lower_red1 = np.array([0, 100, 100])\n","    upper_red1 = np.array([10, 255, 255])\n","    lower_red2 = np.array([160, 100, 100])\n","    upper_red2 = np.array([180, 255, 255])\n","    lower_yellow = np.array([20, 150, 150])\n","    upper_yellow = np.array([40, 255, 255])\n","    lower_green = np.array([40, 50, 50])\n","    upper_green = np.array([90, 255, 255])\n","    return (lower_red1, upper_red1), (lower_red2, upper_red2), (lower_yellow, upper_yellow), (lower_green, upper_green)\n","\n","# Detect traffic light\n","def detect_traffic_light(frame):\n","    hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n","    (lr1, ur1), (lr2, ur2), (ly, uy), (lg, ug) = define_color_ranges()\n","\n","    red_mask = cv2.inRange(hsv_frame, lr1, ur1) | cv2.inRange(hsv_frame, lr2, ur2)\n","    yellow_mask = cv2.inRange(hsv_frame, ly, uy)\n","    green_mask = cv2.inRange(hsv_frame, lg, ug)\n","\n","    kernel = np.ones((5, 5), np.uint8)\n","    red_mask = cv2.morphologyEx(red_mask, cv2.MORPH_OPEN, kernel)\n","    yellow_mask = cv2.morphologyEx(yellow_mask, cv2.MORPH_OPEN, kernel)\n","    green_mask = cv2.morphologyEx(green_mask, cv2.MORPH_OPEN, kernel)\n","\n","    contours_red = cv2.findContours(red_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0]\n","    contours_yellow = cv2.findContours(yellow_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0]\n","    contours_green = cv2.findContours(green_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0]\n","\n","    color = \"None\"\n","    contours = None\n","    min_area = 500\n","\n","    for cnts, col in [(contours_red, \"Red, Stop\"),\n","                     (contours_yellow, \"Yellow, Slow Down\"),\n","                     (contours_green, \"Green, Go\")]:\n","        if cnts:\n","            largest = max(cnts, key=cv2.contourArea)\n","            if cv2.contourArea(largest) > min_area:\n","                color, contours = col, cnts\n","                break\n","\n","    return color, contours\n","\n","# Draw bounding box\n","def draw_traffic_light_box(frame, color, contours):\n","    if contours:\n","        largest = max(contours, key=cv2.contourArea)\n","        x, y, w, h = cv2.boundingRect(largest)\n","        colors = {\n","            \"Red, Stop\": (0, 0, 255),\n","            \"Yellow, Slow Down\": (0, 255, 255),\n","            \"Green, Go\": (0, 255, 0)\n","        }\n","        cv2.rectangle(frame, (x, y), (x + w, y + h), colors.get(color, (255, 255, 255)), 3)\n","        cv2.putText(frame, color, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, colors.get(color, (255, 255, 255)), 2)\n","    return frame\n","\n","# Detect lanes\n","def detect_lanes(frame):\n","    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n","    edges = cv2.Canny(blurred, 50, 150)\n","\n","    height, width = edges.shape\n","    mask = np.zeros_like(edges)\n","    roi = np.array([[(0, height), (width//2, height//2), (width, height)]], dtype=np.int32)\n","    cv2.fillPoly(mask, roi, 255)\n","    masked_edges = cv2.bitwise_and(edges, mask)\n","\n","    lines = cv2.HoughLinesP(masked_edges, 2, np.pi/180, 100, minLineLength=40, maxLineGap=20)\n","    line_image = np.zeros_like(frame)\n","\n","    if lines is not None:\n","        for line in lines:\n","            x1, y1, x2, y2 = line[0]\n","            cv2.line(line_image, (x1, y1), (x2, y2), (0, 0, 255), 5)\n","\n","    return cv2.addWeighted(frame, 0.8, line_image, 1, 0)\n","\n","# Streamlit UI\n","st.title(\"üö¶ Traffic Light & Lane Detection üõ£Ô∏è\")\n","st.write(\"Upload a video or image to detect traffic lights and lanes\")\n","\n","uploaded_file = st.file_uploader(\"Choose a file\", type=[\"mp4\", \"avi\", \"mov\", \"jpg\", \"png\"])\n","\n","if uploaded_file is not None:\n","    tfile = tempfile.NamedTemporaryFile(delete=False)\n","    tfile.write(uploaded_file.read())\n","\n","    if uploaded_file.type.startswith('image'):\n","        # Process image\n","        frame = cv2.imread(tfile.name)\n","        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","\n","        color, contours = detect_traffic_light(frame)\n","        frame = draw_traffic_light_box(frame, color, contours)\n","        frame = detect_lanes(frame)\n","\n","        st.image(frame, caption=\"Processed Image\", use_column_width=True)\n","        st.success(f\"Detected: {color if color != 'None' else 'No traffic light detected'}\")\n","\n","    else:\n","        # Process video\n","        cap = cv2.VideoCapture(tfile.name)\n","        stframe = st.empty()\n","\n","        while cap.isOpened():\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","\n","            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            color, contours = detect_traffic_light(frame)\n","            frame = draw_traffic_light_box(frame, color, contours)\n","            frame = detect_lanes(frame)\n","\n","            stframe.image(frame, caption=\"Processing...\", use_column_width=True)\n","\n","        cap.release()\n","        st.success(\"Video processing complete!\")\n","\n","    os.unlink(tfile.name)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"KH-J-4Vko2KC","outputId":"ae460cdc-6986-4dc4-9f2b-22281e95efd1"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†ºUsage: streamlit run [OPTIONS] TARGET [ARGS]...\n","Try 'streamlit run --help' for help.\n","\n","Error: Invalid value: File does not exist: app.py\n","\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K\u001b[1G\u001b[0JNeed to install the following packages:\n","localtunnel@2.0.2\n","Ok to proceed? (y) \u001b[20Gy\n","\n","\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0Kyour url is: https://funny-cases-cover.loca.lt\n","/root/.npm/_npx/75ac80b86e83d4a2/node_modules/localtunnel/bin/lt.js:81\n","    throw err;\n","    ^\n","\n","Error: connection refused: localtunnel.me:27163 (check your firewall settings)\n","    at Socket.<anonymous> (/root/.npm/_npx/75ac80b86e83d4a2/node_modules/\u001b[4mlocaltunnel\u001b[24m/lib/TunnelCluster.js:52:11)\n","\u001b[90m    at Socket.emit (node:events:524:28)\u001b[39m\n","\u001b[90m    at emitErrorNT (node:internal/streams/destroy:169:8)\u001b[39m\n","\u001b[90m    at emitErrorCloseNT (node:internal/streams/destroy:128:3)\u001b[39m\n","\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:82:21)\u001b[39m\n","\n","Node.js v20.19.0\n","\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K"]}],"source":["!streamlit run app.py & npx localtunnel --port 8501"]},{"cell_type":"code","source":["# Install dependencies if not already installed\n","!pip install -q ipywidgets opencv-python-headless\n","\n","import cv2\n","import numpy as np\n","import io\n","import os\n","from base64 import b64encode\n","from IPython.display import display, HTML\n","import ipywidgets as widgets\n","from google.colab import files\n","\n","# Define HSV color ranges\n","def define_color_ranges():\n","    lower_red1 = np.array([0, 100, 100])\n","    upper_red1 = np.array([10, 255, 255])\n","    lower_red2 = np.array([160, 100, 100])\n","    upper_red2 = np.array([180, 255, 255])\n","    lower_yellow = np.array([20, 150, 150])\n","    upper_yellow = np.array([40, 255, 255])\n","    lower_green = np.array([40, 50, 50])\n","    upper_green = np.array([90, 255, 255])\n","    return (lower_red1, upper_red1), (lower_red2, upper_red2), (lower_yellow, upper_yellow), (lower_green, upper_green)\n","\n","# Traffic light detection\n","def detect_traffic_light(frame):\n","    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n","    (lr1, ur1), (lr2, ur2), (ly, uy), (lg, ug) = define_color_ranges()\n","    red_mask = cv2.inRange(hsv, lr1, ur1) | cv2.inRange(hsv, lr2, ur2)\n","    yellow_mask = cv2.inRange(hsv, ly, uy)\n","    green_mask = cv2.inRange(hsv, lg, ug)\n","\n","    kernel = np.ones((5, 5), np.uint8)\n","    red_mask = cv2.dilate(cv2.erode(red_mask, kernel), kernel)\n","    yellow_mask = cv2.dilate(cv2.erode(yellow_mask, kernel), kernel)\n","    green_mask = cv2.dilate(cv2.erode(green_mask, kernel), kernel)\n","\n","    contours_red, _ = cv2.findContours(red_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n","    contours_yellow, _ = cv2.findContours(yellow_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n","    contours_green, _ = cv2.findContours(green_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n","\n","    color = \"None\"\n","    contours = None\n","\n","    if contours_red and max([cv2.contourArea(c) for c in contours_red]) > 500:\n","        return \"Red, Stop\", contours_red\n","    elif contours_yellow and max([cv2.contourArea(c) for c in contours_yellow]) > 500:\n","        return \"Yellow, Slow Down\", contours_yellow\n","    elif contours_green and max([cv2.contourArea(c) for c in contours_green]) > 500:\n","        return \"Green, Go\", contours_green\n","    else:\n","        return color, contours\n","\n","# Draw bounding box\n","'''def draw_traffic_light_box(frame, color, contours):\n","    if contours:\n","        largest = max(contours, key=cv2.contourArea)\n","        x, y, w, h = cv2.boundingRect(largest)\n","        color_map = {\n","            \"Red, Stop\": (0, 0, 255),\n","            \"Yellow, Slow Down\": (0, 255, 255),\n","            \"Green, Go\": (0, 255, 0),\n","            \"None\": (255, 255, 255)\n","        }\n","        cv2.rectangle(frame, (x, y), (x+w, y+h), color_map.get(color, (255,255,255)), 4)\n","    return frame\n","'''\n","def detect_traffic_light(frame):\n","    # Focus on upper part of the image\n","    height, width, _ = frame.shape\n","    roi = frame[0:int(height * 0.5), :]  # Top half only\n","    hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n","\n","    (lr1, ur1), (lr2, ur2), (ly, uy), (lg, ug) = define_color_ranges()\n","\n","    # Color masks\n","    red_mask = cv2.inRange(hsv, lr1, ur1) | cv2.inRange(hsv, lr2, ur2)\n","    yellow_mask = cv2.inRange(hsv, ly, uy)\n","    green_mask = cv2.inRange(hsv, lg, ug)\n","\n","    # Morphological operations\n","    kernel = np.ones((5, 5), np.uint8)\n","    red_mask = cv2.dilate(cv2.erode(red_mask, kernel), kernel)\n","    yellow_mask = cv2.dilate(cv2.erode(yellow_mask, kernel), kernel)\n","    green_mask = cv2.dilate(cv2.erode(green_mask, kernel), kernel)\n","\n","    # Circle detection helper\n","    def detect_circles(mask):\n","        circles = cv2.HoughCircles(mask, cv2.HOUGH_GRADIENT, dp=1.2, minDist=20,\n","                                   param1=50, param2=15, minRadius=5, maxRadius=50)\n","        return circles\n","\n","    # Try to detect circles in each color\n","    red_circles = detect_circles(red_mask)\n","    yellow_circles = detect_circles(yellow_mask)\n","    green_circles = detect_circles(green_mask)\n","\n","    if red_circles is not None:\n","        return \"Red, Stop\", [np.array([[int(c[0]), int(c[1])]]) for c in red_circles[0]]\n","    elif yellow_circles is not None:\n","        return \"Yellow, Slow Down\", [np.array([[int(c[0]), int(c[1])]]) for c in yellow_circles[0]]\n","    elif green_circles is not None:\n","        return \"Green, Go\", [np.array([[int(c[0]), int(c[1])]]) for c in green_circles[0]]\n","    else:\n","        return \"None\", None\n","\n","# Lane detection\n","def detect_lanes(frame):\n","    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","    blur = cv2.GaussianBlur(gray, (5,5), 0)\n","    edges = cv2.Canny(blur, 50, 150)\n","\n","    height, width = edges.shape\n","    roi = np.array([[(0, height), (width // 2, height // 2), (width, height)]], dtype=np.int32)\n","    mask = np.zeros_like(edges)\n","    cv2.fillPoly(mask, roi, 255)\n","    masked = cv2.bitwise_and(edges, mask)\n","\n","    lines = cv2.HoughLinesP(masked, 2, np.pi/180, 100, minLineLength=40, maxLineGap=20)\n","    line_img = np.zeros_like(frame)\n","\n","    if lines is not None:\n","        for l in lines:\n","            x1, y1, x2, y2 = l[0]\n","            cv2.line(line_img, (x1, y1), (x2, y2), (0,0,255), 5)\n","    return cv2.addWeighted(frame, 0.8, line_img, 1, 0)\n","\n","# Main video processing function\n","def process_video(input_path, output_path='output_result.mp4'):\n","    cap = cv2.VideoCapture(input_path)\n","    width, height = int(cap.get(3)), int(cap.get(4))\n","    fps = int(cap.get(5))\n","    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n","\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        color, contours = detect_traffic_light(frame)\n","        frame = draw_traffic_light_box(frame, color, contours)\n","        frame = detect_lanes(frame)\n","        cv2.putText(frame, f'Traffic Light: {color}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)\n","        out.write(frame)\n","\n","    cap.release()\n","    out.release()\n","\n","# Display video HTML\n","def display_video(file_path):\n","    mp4 = open(file_path,'rb').read()\n","    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n","    display(HTML(f\"\"\"<video width=700 controls><source src=\"{data_url}\" type=\"video/mp4\"></video>\"\"\"))\n","\n","# Upload widget\n","upload_widget = widgets.FileUpload(accept='.mp4', multiple=False)\n","process_button = widgets.Button(description=\"Process Video\")\n","output_area = widgets.Output()\n","\n","def on_process_clicked(b):\n","    with output_area:\n","        output_area.clear_output()\n","        if upload_widget.value:\n","            uploaded_filename = list(upload_widget.value.keys())[0]\n","            uploaded_data = upload_widget.value[uploaded_filename]['content']\n","            input_path = f'/content/{uploaded_filename}'\n","            with open(input_path, 'wb') as f:\n","                f.write(uploaded_data)\n","\n","            print(\"Processing...\")\n","            process_video(input_path)\n","            print(\"Done! Here's the result:\")\n","            display_video(\"output_result.mp4\")\n","        else:\n","            print(\"Please upload a video first.\")\n","\n","process_button.on_click(on_process_clicked)\n","\n","# Display frontend UI\n","display(widgets.VBox([widgets.Label(\"Upload a .mp4 dashcam video:\"), upload_widget, process_button, output_area]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["5f5773ee025e4ecdba60677cf0f56608","0a5efb1d54f5452c9b491126a772e6ed","935c25ff617142429881c06b15603765","8520c3c5540442b8b568623150029361","3f1b1a0fe4784ea48c975e3b7c5ff6e9","e3a6da4904544797bdcd1ecf86cc2acb","131fc40516584ef79ad815afbbdb49af","ebb2a4c36c4f45ed92833350d5924195","cdd4e5aeb5924df79f876516f0b3de63","5f8ee76d57734b7684d05531cd1a3315","111ef14307684b60a047e682f0ef51b2","a5ae68ab306b4264917de14e23637f8d","2163e5daf5f1477a821aac0fc3d2cf0b"]},"id":"_k7zLkpbjcrJ","executionInfo":{"status":"ok","timestamp":1748328830003,"user_tz":-330,"elapsed":4910,"user":{"displayName":"Darsh Verma","userId":"04546128917656418162"}},"outputId":"5b55f3d4-e4f8-4b53-8829-a73edf8a2c01"},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='Upload a .mp4 dashcam video:'), FileUpload(value={}, accept='.mp4', description='U‚Ä¶"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f5773ee025e4ecdba60677cf0f56608"}},"metadata":{}}]},{"cell_type":"code","source":["# Install dependencies if not already installed\n","!pip install -q ipywidgets opencv-python-headless\n","\n","import cv2\n","import numpy as np\n","import io\n","import os\n","from base64 import b64encode\n","from IPython.display import display, HTML\n","import ipywidgets as widgets\n","from google.colab import files\n","\n","# Traffic light memory state\n","class TrafficLightMemory:\n","    def __init__(self):\n","        self.last_color = \"None\"\n","        self.frame_count = 0\n","        self.green_hold_duration = 20  # Hold green signal for 20 frames\n","\n","memory = TrafficLightMemory()\n","\n","# Define HSV color ranges\n","def define_color_ranges():\n","    lower_red1 = np.array([0, 100, 100])\n","    upper_red1 = np.array([10, 255, 255])\n","    lower_red2 = np.array([160, 100, 100])\n","    upper_red2 = np.array([180, 255, 255])\n","    lower_yellow = np.array([20, 150, 150])\n","    upper_yellow = np.array([40, 255, 255])\n","    lower_green = np.array([40, 50, 50])\n","    upper_green = np.array([90, 255, 255])\n","    return (lower_red1, upper_red1), (lower_red2, upper_red2), (lower_yellow, upper_yellow), (lower_green, upper_green)\n","\n","# Traffic light detection using circle detection\n","def detect_traffic_light(frame):\n","    height, width, _ = frame.shape\n","    roi = frame[0:int(height * 0.5), :]  # Top half only\n","    hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n","\n","    (lr1, ur1), (lr2, ur2), (ly, uy), (lg, ug) = define_color_ranges()\n","    red_mask = cv2.inRange(hsv, lr1, ur1) | cv2.inRange(hsv, lr2, ur2)\n","    yellow_mask = cv2.inRange(hsv, ly, uy)\n","    green_mask = cv2.inRange(hsv, lg, ug)\n","\n","    kernel = np.ones((5, 5), np.uint8)\n","    red_mask = cv2.dilate(cv2.erode(red_mask, kernel), kernel)\n","    yellow_mask = cv2.dilate(cv2.erode(yellow_mask, kernel), kernel)\n","    green_mask = cv2.dilate(cv2.erode(green_mask, kernel), kernel)\n","\n","    def detect_circles(mask):\n","        circles = cv2.HoughCircles(mask, cv2.HOUGH_GRADIENT, dp=1.2, minDist=20,\n","                                   param1=50, param2=15, minRadius=5, maxRadius=50)\n","        return circles\n","\n","    red_circles = detect_circles(red_mask)\n","    yellow_circles = detect_circles(yellow_mask)\n","    green_circles = detect_circles(green_mask)\n","\n","    if red_circles is not None:\n","        return \"Red, Stop\", [np.array([[int(c[0]), int(c[1])]]) for c in red_circles[0]]\n","    elif yellow_circles is not None:\n","        return \"Yellow, Slow Down\", [np.array([[int(c[0]), int(c[1])]]) for c in yellow_circles[0]]\n","    elif green_circles is not None:\n","        return \"Green, Go\", [np.array([[int(c[0]), int(c[1])]]) for c in green_circles[0]]\n","    else:\n","        return \"None\", None\n","\n","# Draw bounding box (optional)\n","def draw_traffic_light_box(frame, color, contours):\n","    if contours:\n","        largest = max(contours, key=lambda cnt: cv2.boundingRect(cnt)[2] * cv2.boundingRect(cnt)[3])\n","        x, y, w, h = cv2.boundingRect(largest)\n","        color_map = {\n","            \"Red, Stop\": (0, 0, 255),\n","            \"Yellow, Slow Down\": (0, 255, 255),\n","            \"Green, Go\": (0, 255, 0),\n","            \"None\": (255, 255, 255)\n","        }\n","        cv2.rectangle(frame, (x, y), (x + w, y + h), color_map.get(color, (255, 255, 255)), 4)\n","    return frame\n","\n","# Lane detection\n","def detect_lanes(frame):\n","    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","    blur = cv2.GaussianBlur(gray, (5, 5), 0)\n","    edges = cv2.Canny(blur, 50, 150)\n","\n","    height, width = edges.shape\n","    roi = np.array([[(0, height), (width // 2, height // 2), (width, height)]], dtype=np.int32)\n","    mask = np.zeros_like(edges)\n","    cv2.fillPoly(mask, roi, 255)\n","    masked = cv2.bitwise_and(edges, mask)\n","\n","    lines = cv2.HoughLinesP(masked, 2, np.pi / 180, 100, minLineLength=40, maxLineGap=20)\n","    line_img = np.zeros_like(frame)\n","\n","    if lines is not None:\n","        for l in lines:\n","            x1, y1, x2, y2 = l[0]\n","            cv2.line(line_img, (x1, y1), (x2, y2), (0, 0, 255), 5)\n","    return cv2.addWeighted(frame, 0.8, line_img, 1, 0)\n","\n","# Main video processing function\n","def process_video(input_path, output_path='output_result.mp4'):\n","    cap = cv2.VideoCapture(input_path)\n","    width, height = int(cap.get(3)), int(cap.get(4))\n","    fps = int(cap.get(5))\n","    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n","\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        detected_color, contours = detect_traffic_light(frame)\n","\n","        # Memory logic to hold green signal longer\n","        if detected_color == \"Green, Go\":\n","            memory.last_color = \"Green, Go\"\n","            memory.frame_count = memory.green_hold_duration\n","        elif detected_color != \"None\":\n","            memory.last_color = detected_color\n","            memory.frame_count = 0\n","        elif memory.frame_count > 0:\n","            memory.frame_count -= 1\n","            detected_color = memory.last_color\n","        else:\n","            memory.last_color = \"None\"\n","            detected_color = \"None\"\n","\n","        # Draw detection and lane lines\n","        frame = draw_traffic_light_box(frame, detected_color, contours)\n","        frame = detect_lanes(frame)\n","\n","        # Display result text\n","        cv2.putText(frame, f'Traffic Light: {detected_color}', (10, 30),\n","                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n","\n","        out.write(frame)\n","\n","    cap.release()\n","    out.release()\n","\n","# Display video in notebook\n","def display_video(file_path):\n","    mp4 = open(file_path, 'rb').read()\n","    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n","    display(HTML(f\"\"\"<video width=700 controls><source src=\"{data_url}\" type=\"video/mp4\"></video>\"\"\"))\n","\n","# Upload and process UI\n","upload_widget = widgets.FileUpload(accept='.mp4', multiple=False)\n","process_button = widgets.Button(description=\"Process Video\")\n","output_area = widgets.Output()\n","\n","def on_process_clicked(b):\n","    with output_area:\n","        output_area.clear_output()\n","        if upload_widget.value:\n","            uploaded_filename = list(upload_widget.value.keys())[0]\n","            uploaded_data = upload_widget.value[uploaded_filename]['content']\n","            input_path = f'/content/{uploaded_filename}'\n","            with open(input_path, 'wb') as f:\n","                f.write(uploaded_data)\n","\n","            print(\"Processing...\")\n","            process_video(input_path)\n","            print(\"Done! Here's the result:\")\n","            display_video(\"output_result.mp4\")\n","        else:\n","            print(\"Please upload a video first.\")\n","\n","process_button.on_click(on_process_clicked)\n","\n","# Display UI\n","display(widgets.VBox([widgets.Label(\"Upload a .mp4 dashcam video:\"), upload_widget, process_button, output_area]))\n","\n","files.download('output_result.mp4')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":411,"referenced_widgets":["5e8bbff030224dfd9a14de9d3525292a","4f7fde5451794519955c19c322223160","f8030dfa23ca4651b1548f82d287d228","49150d093e614009b70387882e0503a4","7bec0c34a5ad406491adf94161f36dc7","156725b8382a4ce4803216ccda907d5a","d0144afc5ee349979e866db03a5e73b6","5f26b437dfbc48989915845a3da4d8ae","859a2d9b0f78469fab5b85c47c24a643","ab600d89bcbb47dab6e1b4675622b592","3f0054919b5444408c22e10167afcd45","7b81c105a24040b09c8c8e749e1a8c52","f65a10a3937345a8a07cad0d13ef417a"]},"id":"CsRjqUWopbEj","executionInfo":{"status":"error","timestamp":1748318513636,"user_tz":-330,"elapsed":4701,"user":{"displayName":"Darsh Verma","userId":"04546128917656418162"}},"outputId":"351ba028-267b-4a0b-95d0-979d773f87da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.5/1.6 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='Upload a .mp4 dashcam video:'), FileUpload(value={}, accept='.mp4', description='U‚Ä¶"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e8bbff030224dfd9a14de9d3525292a"}},"metadata":{}},{"output_type":"error","ename":"FileNotFoundError","evalue":"Cannot find file: output_result.mp4","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-9e64ea640859>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidgets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVBox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwidgets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Upload a .mp4 dashcam video:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupload_widget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess_button\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_area\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output_result.mp4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    231\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Cannot find file: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=undefined-variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m   \u001b[0mcomm_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_IPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomm_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: Cannot find file: output_result.mp4"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"5e8bbff030224dfd9a14de9d3525292a":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_4f7fde5451794519955c19c322223160","IPY_MODEL_f8030dfa23ca4651b1548f82d287d228","IPY_MODEL_49150d093e614009b70387882e0503a4","IPY_MODEL_7bec0c34a5ad406491adf94161f36dc7"],"layout":"IPY_MODEL_156725b8382a4ce4803216ccda907d5a"}},"4f7fde5451794519955c19c322223160":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0144afc5ee349979e866db03a5e73b6","placeholder":"‚Äã","style":"IPY_MODEL_5f26b437dfbc48989915845a3da4d8ae","value":"Upload a .mp4 dashcam video:"}},"f8030dfa23ca4651b1548f82d287d228":{"model_module":"@jupyter-widgets/controls","model_name":"FileUploadModel","model_module_version":"1.5.0","state":{"_counter":0,"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FileUploadModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"FileUploadView","accept":".mp4","button_style":"","data":[],"description":"Upload","description_tooltip":null,"disabled":false,"error":"","icon":"upload","layout":"IPY_MODEL_859a2d9b0f78469fab5b85c47c24a643","metadata":[],"multiple":false,"style":"IPY_MODEL_ab600d89bcbb47dab6e1b4675622b592"}},"49150d093e614009b70387882e0503a4":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Process Video","disabled":false,"icon":"","layout":"IPY_MODEL_3f0054919b5444408c22e10167afcd45","style":"IPY_MODEL_7b81c105a24040b09c8c8e749e1a8c52","tooltip":""}},"7bec0c34a5ad406491adf94161f36dc7":{"model_module":"@jupyter-widgets/output","model_name":"OutputModel","model_module_version":"1.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_f65a10a3937345a8a07cad0d13ef417a","msg_id":"","outputs":[]}},"156725b8382a4ce4803216ccda907d5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0144afc5ee349979e866db03a5e73b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f26b437dfbc48989915845a3da4d8ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"859a2d9b0f78469fab5b85c47c24a643":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab600d89bcbb47dab6e1b4675622b592":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"3f0054919b5444408c22e10167afcd45":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b81c105a24040b09c8c8e749e1a8c52":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"f65a10a3937345a8a07cad0d13ef417a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f5773ee025e4ecdba60677cf0f56608":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_0a5efb1d54f5452c9b491126a772e6ed","IPY_MODEL_935c25ff617142429881c06b15603765","IPY_MODEL_8520c3c5540442b8b568623150029361","IPY_MODEL_3f1b1a0fe4784ea48c975e3b7c5ff6e9"],"layout":"IPY_MODEL_e3a6da4904544797bdcd1ecf86cc2acb"}},"0a5efb1d54f5452c9b491126a772e6ed":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_131fc40516584ef79ad815afbbdb49af","placeholder":"‚Äã","style":"IPY_MODEL_ebb2a4c36c4f45ed92833350d5924195","value":"Upload a .mp4 dashcam video:"}},"935c25ff617142429881c06b15603765":{"model_module":"@jupyter-widgets/controls","model_name":"FileUploadModel","model_module_version":"1.5.0","state":{"_counter":0,"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FileUploadModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"FileUploadView","accept":".mp4","button_style":"","data":[],"description":"Upload","description_tooltip":null,"disabled":false,"error":"","icon":"upload","layout":"IPY_MODEL_cdd4e5aeb5924df79f876516f0b3de63","metadata":[],"multiple":false,"style":"IPY_MODEL_5f8ee76d57734b7684d05531cd1a3315"}},"8520c3c5540442b8b568623150029361":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Process Video","disabled":false,"icon":"","layout":"IPY_MODEL_111ef14307684b60a047e682f0ef51b2","style":"IPY_MODEL_a5ae68ab306b4264917de14e23637f8d","tooltip":""}},"3f1b1a0fe4784ea48c975e3b7c5ff6e9":{"model_module":"@jupyter-widgets/output","model_name":"OutputModel","model_module_version":"1.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_2163e5daf5f1477a821aac0fc3d2cf0b","msg_id":"","outputs":[]}},"e3a6da4904544797bdcd1ecf86cc2acb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"131fc40516584ef79ad815afbbdb49af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ebb2a4c36c4f45ed92833350d5924195":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cdd4e5aeb5924df79f876516f0b3de63":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f8ee76d57734b7684d05531cd1a3315":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"111ef14307684b60a047e682f0ef51b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5ae68ab306b4264917de14e23637f8d":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"2163e5daf5f1477a821aac0fc3d2cf0b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}